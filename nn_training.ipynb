{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4f0f9-0878-4bd9-a17b-d7b0b570a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict, defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import list_metrics, load_metric, Dataset\n",
    "\n",
    "import emoji\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "DATA_DIR = \"data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11757b81-8121-445f-9a33-fcf50141f1dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Image classifier\n",
    "\n",
    "As a net I'll use simple ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195fdb4-6563-452d-a570-9273cdafb662",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043cb03e-cead-42ae-8ee7-f09bb16a0b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ImageFolder expects images to be separated by classes\n",
    "# It's not the case for us, so we'll have to manually update them\n",
    "with open(f\"{DATA_DIR}/labels.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "no_lbl = set()\n",
    "for img_path in os.listdir(f\"{DATA_DIR}/images\"):\n",
    "    profile_id = img_path.split(\"_\")[0]\n",
    "    # Move labeled photos into two directories with corresponding labels\n",
    "    if os.path.isfile(f\"{DATA_DIR}/images/{img_path}\"):\n",
    "        try:\n",
    "            os.renames(f\"{DATA_DIR}/images/{img_path}\", f\"{DATA_DIR}/images/{labels[profile_id]}/{img_path}\")\n",
    "        except KeyError:\n",
    "            no_lbl.add(profile_id)\n",
    "            \n",
    "print(f\"{len(no_lbl)} profiles don't have a lable yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e73d94-48a4-4ac5-8f45-3de52a258bfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_images(\n",
    "    data_path: str,\n",
    "    batch_size: int, \n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 1,\n",
    "    transformations: list = None,\n",
    "    train_size: float = None,\n",
    "    seed: int = 42\n",
    "):  \n",
    "    transforms = torchvision.transforms.Compose(transformations)\n",
    "    data = torchvision.datasets.ImageFolder(data_path, transforms)\n",
    "    \n",
    "    if train_size is not None:\n",
    "        _train_size = int(len(data) * train_size)\n",
    "        _val_size = len(data) - _train_size\n",
    "\n",
    "        print(\"Train dataset size:\", _train_size, \"test dataset size:\", _val_size)\n",
    "        \n",
    "        gen = torch.Generator().manual_seed(seed)\n",
    "        data_train, data_val = random_split(data, [_train_size, _val_size], generator=gen)\n",
    "\n",
    "        return (\n",
    "            DataLoader(data_train, batch_size, shuffle, num_workers=num_workers),\n",
    "            DataLoader(data_val, batch_size, shuffle, num_workers=num_workers)\n",
    "        )\n",
    "    \n",
    "    return DataLoader(data, batch_size, shuffle, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993839c2-c692-4f43-820b-3a37d2ad7873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load images as ImageFolder dataset\n",
    "# Define the transformations that'll do to images\n",
    "transforms = [\n",
    "    torchvision.transforms.Resize((128, 64)), \n",
    "    torchvision.transforms.RandomGrayscale(0.3),\n",
    "    torchvision.transforms.RandomCrop((96, 48), padding=4),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "]\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_images, val_images = load_images(\n",
    "    f\"{DATA_DIR}/images\",\n",
    "    BATCH_SIZE,\n",
    "    transformations=transforms,\n",
    "    train_size=0.85\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596db67-0618-40c2-81ce-620f46e8feac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97682037-63dd-4da2-9704-a4c5d89e866f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=False)\n",
    "        self.bn_1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn_2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.res_con = None\n",
    "        if stride != 1 and in_channels != out_channels:\n",
    "            self.res_con = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, X):\n",
    "        input_ = X\n",
    "        out = self.bn_1(self.conv_1(X))\n",
    "        out = self.relu(out)\n",
    "        out = self.bn_2(self.conv_2(out))\n",
    "\n",
    "        if self.res_con is not None:\n",
    "            out += self.res_con(input_)\n",
    "\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a828303-f466-49d4-89a9-7da91ce60595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels, layers, num_classes=10):\n",
    "        self.hidden_size = out_channels\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            input_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn_1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.res_1 = self.make_res_block(out_channels, out_channels, layers[0], 1)\n",
    "        self.res_2 = self.make_res_block(out_channels, 128, layers[1], 2)\n",
    "        self.res_3 = self.make_res_block(128, 256, layers[2], 2)\n",
    "        self.res_4 = self.make_res_block(256, 512, layers[3], 2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flat = nn.Flatten()\n",
    "        self.out = nn.Linear(512, num_classes, bias=False)\n",
    "\n",
    "    def make_res_block(self, input_channels, output_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResBlock(self.hidden_size, output_channels, stride))\n",
    "            self.hidden_size = output_channels\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.bn_1(self.conv_1(X))\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.res_1(out)\n",
    "        out = self.res_2(out)\n",
    "        out = self.res_3(out)\n",
    "        out = self.res_4(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = self.flat(out)\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a30f892c-7455-4102-a5f8-f107fd6d035e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "_X = torch.randn((2, 3, 96, 48))\n",
    "_resnet = ResNet(3, 64, [2, 2, 2, 2], num_classes=2)\n",
    "print(_resnet(_X).shape)\n",
    "del _X, _resnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224d79d-a48f-4633-8e2f-6bc8191318f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7549321a-61a3-4fc4-8e97-172f5c7562e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_label: str = None,\n",
    "        y_label: str = None,\n",
    "        legend: list = None,\n",
    "        x_lim: list = None,\n",
    "        y_lim: list = None,\n",
    "        x_scale: str = \"log\",\n",
    "        y_scale: str = \"log\",\n",
    "        n_rows: int = 1,\n",
    "        n_cols: int = 1,\n",
    "        figsize: tuple = (8, 6)\n",
    "    ):\n",
    "\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "        if n_rows * n_cols == 1:\n",
    "            self.axes = [self.axes,]\n",
    "        self.config_axes = lambda: self.set_axes(\n",
    "            self.axes[0], x_label, y_label, x_lim, y_lim, x_scale, y_scale, legend\n",
    "        )\n",
    "        self.X, self.Y = None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "        \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "        axes.set_xlabel(xlabel)\n",
    "        axes.set_ylabel(ylabel)\n",
    "        axes.set_xscale(xscale)\n",
    "        axes.set_yscale(yscale)\n",
    "        axes.set_xlim(xlim)\n",
    "        axes.set_ylim(ylim)\n",
    "        if legend:\n",
    "            axes.legend(legend)\n",
    "        axes.grid()\n",
    "\n",
    "    def add(self, x, y):\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y in zip(self.X, self.Y):\n",
    "            self.axes[0].plot(x, y)\n",
    "        self.config_axes()\n",
    "\n",
    "    def plot(self):\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(self.fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "107a7eea-25bc-4296-9e08-5908303b3585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_weights_(layer):\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(layer.weight, mode=\"fan_out\")\n",
    "    elif isinstance(layer, (nn.BatchNorm2d)):\n",
    "        nn.init.constant_(layer.weight, 1)\n",
    "        nn.init.constant_(layer.bias, 0)\n",
    "    elif isinstance(layer, nn.Linear):\n",
    "        nn.init.normal_(layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf4e4d3f-3555-4536-a6d8-7b5f5e573d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def try_gpu():\n",
    "    if torch.cuda.device_count() > 0:\n",
    "        return torch.device(\"cuda:0\")\n",
    "\n",
    "    return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2af374-e5e2-4974-87c6-496c4cfbc61d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "102c1550-3f0e-42df-a14a-023c30f8178a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    net, train_iter, test_iter, epochs, optim, loss=None, device=None,\n",
    "    init_weights=False, debug=False, save_model=None,\n",
    "    verbose_interval=5, scheduler=None, clip_grad=False\n",
    "    ):\n",
    "    # Init stuff\n",
    "    if init_weights:\n",
    "        net.apply(init_weights_)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss() if not loss else loss\n",
    "    plotter = Plotter(\n",
    "        x_label=\"epochs\", \n",
    "        y_label=\"loss\", \n",
    "        x_lim=[1, epochs], \n",
    "        legend=[\"train loss\", \"test loss\"]\n",
    "    )\n",
    "\n",
    "    num_batches = len(train_iter)\n",
    "\n",
    "    device = try_gpu() if not device else device\n",
    "    print(f\"Training on {device}\")\n",
    "    net.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        train_loss_ = []\n",
    "        \n",
    "        for i, (X, y) in enumerate(tqdm(train_iter)):\n",
    "            optim.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_pred = net(X)\n",
    "            l = loss(y_pred, y)\n",
    "            l.backward()\n",
    "            train_loss_.append(l.item())\n",
    "            \n",
    "            if clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=2., norm_type=2)\n",
    "            optim.step()\n",
    "\n",
    "            if debug:\n",
    "                break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            \n",
    "            test_loss_ = []\n",
    "            for X_test, y_test in test_iter:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                pred_test = net(X_test)\n",
    "                test_loss_.append(loss(pred_test, y_test).item())\n",
    "            \n",
    "            train_loss = np.mean(train_loss_)\n",
    "            test_loss = np.mean(test_loss_)\n",
    "            \n",
    "            plotter.add(epoch + 1, (train_loss, test_loss))\n",
    "\n",
    "        if (epoch + 1) % verbose_interval == 0 or epoch == 0 or epoch == (epochs - 1):\n",
    "                plotter.plot()\n",
    "                print(\n",
    "                    f\"epoch: {epoch}\", f'train loss: {train_loss:.3f}, test loss: {test_loss:.3f}, '\n",
    "                    f\"lr: {optim.param_groups[0]['lr']:.5f}\"\n",
    "                )\n",
    "\n",
    "        if debug:\n",
    "            break\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    if save_model is not None:\n",
    "        torch.save(net.state_dict(), save_model)\n",
    "        print(f\"Model saved to {save_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d81eee94-1f4a-4e96-9280-9e0f96a23763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(resnet_50.state_dict(), \n",
    "           os.path.join(\"/Users/gleb/Documents/GitHub/tinder_swiper/data/models\", \"image_model.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03830bdf-f9ed-40ee-9971-5a6b576d9c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet_50 = ResNet(3, 64, [2, 2, 2, 2], num_classes=2)\n",
    "EPOCHS = 100\n",
    "lr = 0.1\n",
    "WD = 5e-4\n",
    "optim = torch.optim.AdamW(resnet_50.parameters(), lr, weight_decay=WD)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=EPOCHS, eta_min=1e-4)\n",
    "\n",
    "train_model(resnet_50, train_images, val_images, EPOCHS, optim, init_weights=True, scheduler=scheduler, debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531e45d-e52c-42f8-a535-66d3de24e155",
   "metadata": {},
   "source": [
    "## Bio classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e295391-a5d0-45fd-a8c3-5b62b5cc4627",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load and clean bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f1527-247c-4399-9f18-08daf4693ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{DATA_DIR}/bios.pkl\", \"rb\") as f:\n",
    "    bios = pickle.load(f)\n",
    "\n",
    "print(len(bios))\n",
    "dataset_dict = defaultdict(list)\n",
    "unlabeled = []\n",
    "\n",
    "for k, v in bios.items():\n",
    "    if k in labels:\n",
    "        dataset_dict[\"id\"].append(k)\n",
    "        dataset_dict[\"label\"].append(labels[k])\n",
    "        for k_, v_ in v.items():\n",
    "            dataset_dict[k_].append(v_)\n",
    "    else:\n",
    "        unlabeled.append(k)\n",
    "\n",
    "print(f\"There are {len(unlabeled)} unlabeled profiles\")\n",
    "        \n",
    "bios_dataset = Dataset.from_dict(dataset_dict)\n",
    "bios_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862a4a0-7ccf-42e0-b839-99dc6ff9e03e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_bios(bios, bio_col=\"bio\"):\n",
    "    def _cleaner(text):\n",
    "        flags = re.findall(u'[\\U0001F1E6-\\U0001F1FF]', text)\n",
    "        \n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = \"\".join(w for w in text if w not in emoji.EMOJI_DATA and w not in flags)\n",
    "        text = text.replace(\"\\u200d\", \"\")\n",
    "\n",
    "        return text\n",
    "\n",
    "    return {f\"{bio_col}_clean\": _cleaner(bios[bio_col])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda73e6-258b-4828-959d-1bb6299acec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bios_dataset = bios_dataset.map(clean_bios, batched=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f634173a-51bc-471d-b220-e874ce8dd933",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2c906-c98d-4ad8-98f3-33dd516f4f70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(dataset, tokenizer, text_col=\"bio_clean\"):\n",
    "    return tokenizer(dataset[text_col], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "bios_dataset = bios_dataset.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, batched=True)\n",
    "bios_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b461529-efe2-4e72-8efb-a7ce92ba37fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-multilingual-cased\",\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06596fb3-9471-4110-ab0a-597dd987e1af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layers_to_train = [\n",
    "    \"distilbert.transformer.layer.5.ffn.lin1.weight\",\n",
    "    \"distilbert.transformer.layer.5.ffn.lin1.bias\",\n",
    "    \"distilbert.transformer.layer.5.ffn.lin2.weight\",\n",
    "    \"distilbert.transformer.layer.5.ffn.lin2.bias\",\n",
    "    \"distilbert.transformer.layer.5.output_layer_norm.weight\",\n",
    "    \"distilbert.transformer.layer.5.output_layer_norm.bias\",\n",
    "    \"pre_classifier.weight\",\n",
    "    \"pre_classifier.bias\",\n",
    "    \"classifier.weight\",\n",
    "    \"classifier.bias\",\n",
    "]\n",
    "\n",
    "for n, l in bert.named_parameters():\n",
    "    if n not in layers_to_train:\n",
    "        l.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60538962-f8e2-49b8-b553-b5379fe4e26e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bios_dataset = bios_dataset.train_test_split(test_size=0.15)\n",
    "text_dataset = (\n",
    "    bios_dataset.rename_column(\"bio_clean\", \"text\")\n",
    "    .remove_columns([\"id\", \"name\", \"bio\", \"age\"])\n",
    ")\n",
    "text_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72ef83-976d-462a-9e27-145ac6d26cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred, metric=None):\n",
    "    metric = load_metric(\"accuracy\") if metric is None else metric\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.array([np.argmax(logits, axis=-1)]).flatten()\n",
    "    return metric.compute(references=labels, prediction_scores=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb6c6e2-bcf1-449c-be75-b72b2e103ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_trainer\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2_500,\n",
    "    per_device_train_batch_size=16,\n",
    "    weight_decay=3e-10,\n",
    "    num_train_epochs=4,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    save_steps=2_500,\n",
    "    max_steps=12_500\n",
    ")\n",
    "\n",
    "# Create training pipeline\n",
    "trainer = Trainer(\n",
    "    model=bert,\n",
    "    args=training_args,\n",
    "    train_dataset=bios_dataset[\"train\"],\n",
    "    eval_dataset=bios_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "445525aa-108b-42aa-9f58-dfaaf9901ea0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column name nanan not in the dataset. Current columns in the dataset: ['id', 'label', 'name', 'bio', 'age', 'bio_clean', 'input_ids', 'attention_mask']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbios_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnanan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tinder/lib/python3.11/site-packages/datasets/dataset_dict.py:366\u001b[0m, in \u001b[0;36mDatasetDict.remove_columns\u001b[0;34m(self, column_names)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tinder/lib/python3.11/site-packages/datasets/dataset_dict.py:366\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;124;03mRemove one or several column(s) from each split in the dataset\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03mand the features associated to the column(s).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict({k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tinder/lib/python3.11/site-packages/datasets/arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tinder/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tinder/lib/python3.11/site-packages/datasets/fingerprint.py:481\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tinder/lib/python3.11/site-packages/datasets/arrow_dataset.py:2152\u001b[0m, in \u001b[0;36mDataset.remove_columns\u001b[0;34m(self, column_names, new_fingerprint)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m   2151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[0;32m-> 2152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2153\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2154\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2155\u001b[0m         )\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column_name \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m   2158\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures[column_name]\n",
      "\u001b[0;31mValueError\u001b[0m: Column name nanan not in the dataset. Current columns in the dataset: ['id', 'label', 'name', 'bio', 'age', 'bio_clean', 'input_ids', 'attention_mask']"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc319e-9c21-47cd-8990-c668f9dae99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
